#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Intel OpenVINO æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—
å±•ç¤ºä¼˜åŒ–å‰åçš„æ€§èƒ½å¯¹æ¯”ï¼Œæ»¡è¶³è‹±ç‰¹å°”AIå¤§èµ›è¯„åˆ†è¦æ±‚
"""

import time
import psutil
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from optimum.intel.openvino import OVModelForSequenceClassification
import matplotlib.pyplot as plt
import pandas as pd
from openvino import Core
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntelPerformanceBenchmark:
    """Intel AIä¼˜åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {
            'model_type': [],
            'inference_time': [],
            'memory_usage': [],
            'cpu_usage': [],
            'throughput': []
        }
        
    def benchmark_emotion_model(self, text_samples):
        """å¯¹æ¯”åŸå§‹æ¨¡å‹vs OpenVINOä¼˜åŒ–æ¨¡å‹æ€§èƒ½"""
        
        logger.info("ğŸ”¥ å¼€å§‹Intel OpenVINOæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # 1. æµ‹è¯•åŸå§‹PyTorchæ¨¡å‹
        logger.info("ğŸ“Š æµ‹è¯•åŸå§‹PyTorchæ¨¡å‹...")
        pytorch_metrics = self._test_pytorch_model(text_samples)
        
        # 2. æµ‹è¯•OpenVINOä¼˜åŒ–æ¨¡å‹  
        logger.info("âš¡ æµ‹è¯•Intel OpenVINOä¼˜åŒ–æ¨¡å‹...")
        openvino_metrics = self._test_openvino_model(text_samples)
        
        # 3. è®¡ç®—æ€§èƒ½æå‡
        speed_improvement = pytorch_metrics['avg_time'] / openvino_metrics['avg_time']
        memory_reduction = (pytorch_metrics['memory'] - openvino_metrics['memory']) / pytorch_metrics['memory'] * 100
        
        logger.info(f"ğŸš€ Intel OpenVINOä¼˜åŒ–æ•ˆæœ:")
        logger.info(f"   æ¨ç†é€Ÿåº¦æå‡: {speed_improvement:.2f}x")
        logger.info(f"   å†…å­˜ä½¿ç”¨å‡å°‘: {memory_reduction:.1f}%")
        logger.info(f"   CPUä½¿ç”¨ç‡é™ä½: {pytorch_metrics['cpu'] - openvino_metrics['cpu']:.1f}%")
        
        return {
            'pytorch': pytorch_metrics,
            'openvino': openvino_metrics,
            'improvement': {
                'speed': speed_improvement,
                'memory_reduction': memory_reduction
            }
        }
    
    def _test_pytorch_model(self, text_samples):
        """æµ‹è¯•åŸå§‹PyTorchæ¨¡å‹æ€§èƒ½"""
        
        tokenizer = AutoTokenizer.from_pretrained("uer/roberta-base-finetuned-jd-binary-chinese")
        model = AutoModelForSequenceClassification.from_pretrained("uer/roberta-base-finetuned-jd-binary-chinese")
        model.eval()
        
        # é¢„çƒ­
        for _ in range(3):
            inputs = tokenizer("æµ‹è¯•æ–‡æœ¬", return_tensors="pt", truncation=True, padding=True)
            with torch.no_grad():
                _ = model(**inputs)
        
        # æ€§èƒ½æµ‹è¯•
        times = []
        memory_before = psutil.virtual_memory().used / 1024 / 1024  # MB
        cpu_before = psutil.cpu_percent(interval=1)
        
        for text in text_samples:
            start_time = time.time()
            
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
            with torch.no_grad():
                outputs = model(**inputs)
                _ = torch.softmax(outputs.logits, dim=-1)
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        memory_after = psutil.virtual_memory().used / 1024 / 1024  # MB
        cpu_after = psutil.cpu_percent(interval=1)
        
        return {
            'avg_time': np.mean(times),
            'std_time': np.std(times),
            'memory': memory_after - memory_before,
            'cpu': (cpu_before + cpu_after) / 2,
            'throughput': len(text_samples) / sum(times)
        }
    
    def _test_openvino_model(self, text_samples):
        """æµ‹è¯•Intel OpenVINOä¼˜åŒ–æ¨¡å‹æ€§èƒ½"""
        
        tokenizer = AutoTokenizer.from_pretrained("uer/roberta-base-finetuned-jd-binary-chinese")
        
        # åŠ è½½OpenVINOä¼˜åŒ–æ¨¡å‹
        try:
            model = OVModelForSequenceClassification.from_pretrained(
                "models/emotion_openvino", 
                device="GPU"  # ä½¿ç”¨Intel GPUåŠ é€Ÿ
            )
        except:
            # å¦‚æœGPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU
            model = OVModelForSequenceClassification.from_pretrained(
                "models/emotion_openvino", 
                device="CPU"
            )
        
        # é¢„çƒ­
        for _ in range(3):
            inputs = tokenizer("æµ‹è¯•æ–‡æœ¬", return_tensors="pt", truncation=True, padding=True)
            _ = model(**inputs)
        
        # æ€§èƒ½æµ‹è¯•
        times = []
        memory_before = psutil.virtual_memory().used / 1024 / 1024  # MB
        cpu_before = psutil.cpu_percent(interval=1)
        
        for text in text_samples:
            start_time = time.time()
            
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
            outputs = model(**inputs)
            _ = torch.softmax(outputs.logits, dim=-1)
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        memory_after = psutil.virtual_memory().used / 1024 / 1024  # MB
        cpu_after = psutil.cpu_percent(interval=1)
        
        return {
            'avg_time': np.mean(times),
            'std_time': np.std(times),
            'memory': memory_after - memory_before,
            'cpu': (cpu_before + cpu_after) / 2,
            'throughput': len(text_samples) / sum(times)
        }
    
    def generate_benchmark_report(self, results):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        plt.rcParams['font.family'] = 'SimHei'
        
        # 1. æ¨ç†æ—¶é—´å¯¹æ¯”
        models = ['PyTorch', 'Intel OpenVINO']
        times = [results['pytorch']['avg_time'], results['openvino']['avg_time']]
        ax1.bar(models, times, color=['#ff7f0e', '#1f77b4'])
        ax1.set_ylabel('æ¨ç†æ—¶é—´ (ç§’)')
        ax1.set_title('æ¨ç†é€Ÿåº¦å¯¹æ¯”')
        ax1.grid(True, alpha=0.3)
        
        # 2. å†…å­˜ä½¿ç”¨å¯¹æ¯”
        memory = [results['pytorch']['memory'], results['openvino']['memory']]
        ax2.bar(models, memory, color=['#ff7f0e', '#1f77b4'])
        ax2.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
        ax2.set_title('å†…å­˜å ç”¨å¯¹æ¯”')
        ax2.grid(True, alpha=0.3)
        
        # 3. ååé‡å¯¹æ¯”
        throughput = [results['pytorch']['throughput'], results['openvino']['throughput']]
        ax3.bar(models, throughput, color=['#ff7f0e', '#1f77b4'])
        ax3.set_ylabel('ååé‡ (æ ·æœ¬/ç§’)')
        ax3.set_title('å¤„ç†ååé‡å¯¹æ¯”')
        ax3.grid(True, alpha=0.3)
        
        # 4. æ€§èƒ½æå‡æ±‡æ€»
        improvements = ['æ¨ç†é€Ÿåº¦', 'å†…å­˜æ•ˆç‡']
        values = [results['improvement']['speed'], 
                 1 + results['improvement']['memory_reduction']/100]
        ax4.bar(improvements, values, color=['#2ca02c', '#d62728'])
        ax4.set_ylabel('æå‡å€æ•°')
        ax4.set_title('Intel OpenVINOä¼˜åŒ–æ•ˆæœ')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('intel_openvino_benchmark.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     Intel OpenVINO æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ”¥ æµ‹è¯•é¡¹ç›®: NeuraLink Intel AIæƒ…æ„Ÿåˆ†æç³»ç»Ÿ                                    â•‘
â•‘ âš¡ ä¼˜åŒ–æŠ€æœ¯: Intel OpenVINO + GPUåŠ é€Ÿ                                        â•‘
â•‘ ğŸ“Š æµ‹è¯•åœºæ™¯: RoBERTaä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹                                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                æ€§èƒ½å¯¹æ¯”ç»“æœ                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“ˆ æ¨ç†é€Ÿåº¦æå‡:     {results['improvement']['speed']:.2f}x                                          â•‘
â•‘ ğŸ’¾ å†…å­˜ä½¿ç”¨å‡å°‘:     {results['improvement']['memory_reduction']:.1f}%                                         â•‘
â•‘ ğŸ¯ å¹³å‡æ¨ç†æ—¶é—´:                                                              â•‘
â•‘    - PyTorchåŸå§‹:   {results['pytorch']['avg_time']:.4f}s                                    â•‘
â•‘    - OpenVINOä¼˜åŒ–:  {results['openvino']['avg_time']:.4f}s                                   â•‘
â•‘ ğŸš€ å¤„ç†ååé‡:                                                                â•‘
â•‘    - PyTorchåŸå§‹:   {results['pytorch']['throughput']:.2f} æ ·æœ¬/ç§’                              â•‘
â•‘    - OpenVINOä¼˜åŒ–:  {results['openvino']['throughput']:.2f} æ ·æœ¬/ç§’                             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                            IntelæŠ€æœ¯ä¼˜åŠ¿                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ âœ… Intel OpenVINOæ¨¡å‹ä¼˜åŒ–: æ˜¾è‘—æå‡æ¨ç†æ€§èƒ½                                    â•‘
â•‘ âœ… Intel GPUåŠ é€Ÿæ”¯æŒ: å……åˆ†åˆ©ç”¨ç¡¬ä»¶å¹¶è¡Œè®¡ç®—                                     â•‘
â•‘ âœ… å†…å­˜æ•ˆç‡ä¼˜åŒ–: å‡å°‘ç³»ç»Ÿèµ„æºå ç”¨                                             â•‘
â•‘ âœ… è·¨å¹³å°å…¼å®¹æ€§: æ”¯æŒCPU/GPU/NPUå¤šç§åŠ é€Ÿæ–¹æ¡ˆ                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        
        return report

def run_intel_benchmark():
    """è¿è¡ŒIntelæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_samples = [
        "ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œå­¦åˆ°äº†å¾ˆå¤šæ–°çŸ¥è¯†",
        "æˆ‘ä¸æƒ³åšä½œä¸šï¼Œæ„Ÿè§‰å¾ˆç´¯",
        "å’Œæœ‹å‹ä¸€èµ·ç©æ¸¸æˆçœŸå¼€å¿ƒ",
        "çˆ¸çˆ¸å¦ˆå¦ˆåˆåµæ¶äº†ï¼Œæˆ‘å¥½å®³æ€•",
        "è€å¸ˆå¤¸å¥–æˆ‘äº†ï¼Œæˆ‘å¾ˆé«˜å…´",
        "è€ƒè¯•æ²¡è€ƒå¥½ï¼Œæˆ‘å¾ˆéš¾è¿‡",
        "æ˜å¤©è¦å»æ¸¸ä¹å›­ï¼ŒæœŸå¾…å¾—ç¡ä¸ç€",
        "å¼Ÿå¼ŸæŠŠæˆ‘çš„ç©å…·å¼„åäº†ï¼Œæˆ‘å¾ˆç”Ÿæ°”"
    ] * 10  # æ‰©å±•æµ‹è¯•æ ·æœ¬
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = IntelPerformanceBenchmark()
    
    # è¿è¡Œæµ‹è¯•
    results = benchmark.benchmark_emotion_model(test_samples)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = benchmark.generate_benchmark_report(results)
    print(report)
    
    # ä¿å­˜ç»“æœ
    with open('intel_benchmark_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    return results

if __name__ == "__main__":
    # è¿è¡ŒIntel OpenVINOæ€§èƒ½åŸºå‡†æµ‹è¯•
    results = run_intel_benchmark() 