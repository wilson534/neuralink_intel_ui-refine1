import os
import socket
import whisper
import requests
import sounddevice as sd
import scipy.io.wavfile as wav
import pyttsx3
import streamlit as st
from datetime import datetime
import torch
from openvino import Core
from sympy import false
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
from optimum.intel.openvino import OVModelForSequenceClassification
#è‡ªå®šä¹‰ æ¨¡å—
from whisper_ov_runner import transcribe_with_openvino
import json
#å¼•å…¥æ‰£å­åº“
from cozepy import COZE_CN_BASE_URL
from cozepy import Coze, TokenAuth, Message, ChatStatus, MessageContentType
#è¿™é‡Œæ˜¯åŠ å…¥äº†æœ¬åœ°çš„æƒ…ç»ªåˆ†æ
import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from db_logger import init_db,insert_log
import numpy as np
import librosa

# ========== æ¨¡å—1ï¼šå½•éŸ³æ¨¡å— ==========
def record_audio(filename="input.wav", duration=5, samplerate=16000):
    print("\nğŸ™ï¸ æ­£åœ¨å½•éŸ³ä¸­...")
    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)
    sd.wait()
    wav.write(filename, samplerate, recording)
    print("âœ… å½•éŸ³å®Œæˆï¼š", filename)
    return filename

# åŠ ä¸Š NPUï¼ŒGPUè°ƒåº¦
core = Core()


# ========== æ¨¡å—2ï¼šWhisper STT ==========
# whisper_model = whisper.load_model("base")
#
def transcribe_audio(file_path):
    text = transcribe_with_openvino("input.wav")
    return text

# ========== æ¨¡å—3ï¼šè”ç½‘åˆ¤æ–­ ==========
def is_online():
    try:
        socket.create_connection(("8.8.8.8", 53), timeout=2)
        return True
    except OSError:
        return False

# ========== æ¨¡å—4ï¼šæœ¬åœ° LLMï¼ˆOllamaï¼‰ ==========
# def local_llm_query(prompt):
#     try:
#         res = requests.post("http://localhost:11434/api/generate", json={
#             "model": "qwen:7b",
#             "prompt": prompt,
#             "stream": False
#         })
#         return res.json()["response"]
#     except:
#         return "[æœ¬åœ°æ¨¡å‹å“åº”å¤±è´¥]"

def local_llm_query(prompt):
    try:
        res = requests.post("http://localhost:11434/api/generate", json={
            "model": "qwen:7b",
            "prompt": prompt,
            "stream": True
        }, stream=True)

        output = ""
        for line in res.iter_lines(decode_unicode=True):
            if line:
                data = json.loads(line)
                output += data.get("response", "")
        return output if output else "[æ— æµå¼å“åº”]"
    except Exception as e:
        return f"[æµå¼å“åº”å¤±è´¥: {str(e)}]"


# ========== æ¨¡å—5ï¼šå¢å¼ºæƒ…ç»ªè¯†åˆ«ï¼ˆä½¿ç”¨æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ï¼‰ ==========
emo_tokenizer = AutoTokenizer.from_pretrained("uer/roberta-base-finetuned-jd-binary-chinese")
#ä¿å­˜èµ·æ¥
# emo_model = OVModelForSequenceClassification.from_pretrained("uer/roberta-base-finetuned-jd-binary-chinese",export=True)
# emo_model.save_pretrained("models/emotion_openvino")
#å·²ç»åœ¨é¡¹ç›®ä¸­è¿›è¡Œä¿å­˜äº† æ‰€ä»¥ç›´æ¥è°ƒç”¨
emo_model = OVModelForSequenceClassification.from_pretrained("models/emotion_openvino",device="GPU")


def enhanced_emotion_analysis(text):
    inputs = emo_tokenizer(text, return_tensors="pt", truncation=True, padding=True,max_length=2048)
    outputs = emo_model(**inputs)
    probs = F.softmax(outputs.logits, dim=-1)
    emo_label = torch.argmax(probs, dim=-1).item()
    emo_map = {0: "è´Ÿé¢", 1: "æ­£é¢"}
    return emo_map.get(emo_label, "ä¸­æ€§")


def analyze_voice_emotion(filename="input.wav"):
    try:
        y, sr = librosa.load(filename, sr=16000)
        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)

        # ç®€å•é˜ˆå€¼æ¨¡æ‹Ÿåˆ†ç±»é€»è¾‘ï¼ˆå¯æ›¿æ¢ä¸ºæ¨¡å‹ï¼‰
        energy = np.mean(librosa.feature.rms(y=y))
        pitch = np.mean(librosa.yin(y, fmin=80, fmax=400, sr=sr))

        if energy < 0.02:
            return "ä½è½"
        elif pitch > 200:
            return "æ¿€åŠ¨"
        else:
            return "å¹³é™"
    except Exception as e:
        return f"è¯­éŸ³æƒ…ç»ªè¯†åˆ«å¤±è´¥:{str(e)}"

# ========== æ¨¡å—6ï¼šæ„å›¾è¯†åˆ«ä¸å»ºè®®ç”Ÿæˆ ==========
def analyze_intent_and_suggestion(text):
    intent_keywords = {
        "è¯·æ±‚å¸®åŠ©": ["ä¸ä¼š", "æ€ä¹ˆåŠ", "å¸®æˆ‘", "èƒ½ä¸èƒ½", "ä½ çŸ¥é“å—"],
        "è¡¨è¾¾æƒ…ç»ª": ["æˆ‘å¥½éš¾è¿‡", "æˆ‘ç”Ÿæ°”", "æˆ‘ä¸æƒ³", "æˆ‘å®³æ€•"],
        "å¯»æ±‚é™ªä¼´": ["é™ªæˆ‘", "ä½ åœ¨å—", "å’Œæˆ‘ç©", "æˆ‘ä¸€ä¸ªäºº"]
    }
    for intent, keys in intent_keywords.items():
        if any(k in text for k in keys):
            suggestion = f"æ£€æµ‹åˆ°å­©å­å¯èƒ½åœ¨'{intent}'ï¼Œå»ºè®®çˆ¶æ¯ç»™äºˆå…³æ³¨ä¸å¼•å¯¼ã€‚"
            return intent, suggestion
    return "æ™®é€šäº¤æµ", "ç›®å‰æ— éœ€ç‰¹æ®Šå¹²é¢„ã€‚"

# ========== æ¨¡å—7ï¼šTTS ==========
def speak_text(text):
    engine = pyttsx3.init()
    engine.setProperty('rate', 170)
    engine.say(text)
    engine.runAndWait()

# ========== æ¨¡å—8ï¼šCoze Agent äº‘ç«¯è°ƒç”¨ ==========

def clean_coze_reply(raw):
    parts = raw.strip().split("\n", 1)
    if len(parts) == 2 and parts[1].lstrip().startswith("{"):
        return parts[0].strip()
    return raw.strip()


def coze_agent_call(query):
    try:
        api_url = "https://api.coze.cn/open_api/v2/chat"
        payload = {
            "bot_id": "7516852953635455039",  # è¯·æ›¿æ¢æˆä½ çš„ bot_id
            "user": "child_user",
            "query": query,
            "stream": False
        }
        headers = {
            "Authorization": "Bearer pat_t0eI6BgSXzCZZKw9d8FQTA4rfaKAEaTRZO4jt9r6T2euoz6lsN3N3aMNcL1ONKOc",
            "Content-Type": "application/json"
        }
        response = requests.post(api_url, json=payload, headers=headers)
        response.raise_for_status()
        tem = response.json()["messages"][1]["content"]
        tem = clean_coze_reply(tem)
        return tem
    except Exception as e:
        return f"[Coze äº‘ç«¯å“åº”å¤±è´¥ï¼š{str(e)}]"


#======================æ–°æ¨¡å—: å†å²æƒ…ç»ªåˆ†æ ====================
def plot_emotion_trends():
    st.sidebar.empty() #æ–°åŠ å…¥çš„
    conn = sqlite3.connect("dialogue_log.db")
    # df = pd.read_sql_query("SELECT timestamp, text_emotion FROM logs", conn)
    df = pd.read_sql_query("SELECT * FROM logs", conn)  # æŸ¥è¯¢æ‰€æœ‰æ•°æ®
    conn.close()

    if df.empty:
        st.warning("æš‚æ— æƒ…ç»ªæ•°æ®è®°å½•")
        return

    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['week'] = df['timestamp'].dt.to_period("W").astype(str)
    df['month'] = df['timestamp'].dt.to_period("M").astype(str)

    st.subheader("ğŸ“ˆ æƒ…ç»ªè¶‹åŠ¿åˆ†æ")

    emotion_counts_week = df.groupby(['week', 'text_emotion']).size().reset_index(name='count')
    emotion_counts_month = df.groupby(['month', 'text_emotion']).size().reset_index(name='count')

    tab1, tab2 = st.tabs(["ğŸ“… å‘¨è§†å›¾", "ğŸ—“ æœˆè§†å›¾"])
    # è®¾ç½®Matplotlibçš„å­—ä½“å‚æ•°
    plt.rcParams['font.family'] = 'SimHei'  # é€‰æ‹©ä¸€ä¸ªæ”¯æŒä¸­æ–‡çš„å­—ä½“
    with tab1:
        fig1 = plt.figure(figsize=(8,4))
        sns.lineplot(data=emotion_counts_week, x='week', y='count', hue='text_emotion', marker='o')
        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig1)

    with tab2:
        fig2 = plt.figure(figsize=(6,4))
        sns.barplot(data=emotion_counts_month, x='month', y='count', hue='text_emotion')
        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig2)


#==========================æ–°åŠ å…¥ å­˜å‚¨æƒ…ç»ª=====================
def show_logs():
    conn = sqlite3.connect("dialogue_log.db")
    df = pd.read_sql_query("SELECT * FROM logs ORDER BY timestamp DESC", conn)
    st.dataframe(df)


# ========== æ¨¡å—9ï¼šStreamlit ç•Œé¢ ==========
st.title("ğŸ‘¨â€ğŸ‘©â€ğŸ‘§ NeruaLinkç¦»çº¿/åœ¨çº¿äº²å­å¯¹è¯åˆ†æç³»ç»Ÿ")
st.caption("Whisper + Ollama + Coze + æƒ…ç»ªè¯†åˆ« + æ„å›¾åˆ†æ + TTS")
init_db() #ç¨‹åºå¼€å§‹çš„æ—¶å€™ åˆå§‹åŒ–ä¸€é
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []

col1, col2 = st.columns(2)
mode = col1.radio("æ¨¡å¼é€‰æ‹©ï¼š", ["è‡ªåŠ¨åˆ‡æ¢ï¼ˆæ¨èï¼‰", "å¼ºåˆ¶ç¦»çº¿", "å¼ºåˆ¶åœ¨çº¿"])
duration = col2.slider("å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰", 2, 10, 5)

if st.button("ğŸ™ï¸ å¼€å§‹å¯¹è¯"):
    filename = record_audio(duration=duration)
    text = transcribe_audio(filename)

    if mode == "å¼ºåˆ¶ç¦»çº¿":
        online = False
    elif mode == "å¼ºåˆ¶åœ¨çº¿":
        online = True
    else:
        online = is_online()

    if online:
        reply = coze_agent_call(text)
    else:
        reply = local_llm_query(text)

    emotion = enhanced_emotion_analysis(text)
    intent, suggestion = analyze_intent_and_suggestion(text)
    speak_text(reply)
    voice_emotion = analyze_voice_emotion(filename)
    # ä¿®æ”¹è¿™ä¸€è¡Œ
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # æ·»åŠ å¹´æœˆæ—¥
    st.session_state.chat_log.append((timestamp, text, reply, emotion,voice_emotion,intent, suggestion))
    insert_log(
        timestamp,
        text,
        reply,
        emotion,
        voice_emotion,
        intent,
        suggestion
    )

# åœ¨ StreamLit ä¸»é¡µé¢ä¸Šä¸­æ·»åŠ äº†å…¥å£æŒ‰é’®
if st.sidebar.button("ğŸ“Š æŸ¥çœ‹æƒ…ç»ªè¶‹åŠ¿å›¾"):
    plot_emotion_trends()

if st.sidebar.button("ğŸ“œ æŸ¥çœ‹å†å²è®°å½•"):
    show_logs()

st.subheader("ğŸ§¾ å¯¹è¯è®°å½•")
for t, user, bot, emo, voice_emo, intent, sugg in reversed(st.session_state.chat_log):
    st.markdown(f"**ğŸ•’ {t}**")
    st.markdown(f"ğŸ‘§ å­©å­è¯´ï¼š`{user}`")
    st.markdown(f"ğŸ¤– å›å¤ï¼š{bot}")
    st.markdown(f"ğŸ§  æ–‡æœ¬æƒ…ç»ªï¼š{emo} ï½œ ğŸ”Š è¯­éŸ³æƒ…ç»ªï¼š{voice_emo}")
    st.markdown(f"ğŸ§© æ„å›¾ï¼š{intent}")
    st.markdown(f"ğŸ“Œ å»ºè®®ï¼š*{sugg}*")
    st.markdown("---")

